[H1] Understand How Ground Your Language Model## Problem You want ensure LLM-based agent provides accurate, domain-specific, factual responses. How ground language model data improve reliability?## Solution Azure UseRetrieval Augmented Generation (RAG)ground language model. RAG retrieves relevant data based users prompt, augments prompt data, uses LLM generate grounded response.## Required Components - Azure AI Foundry project - Grounding data (e.g., documents, files, datasets) - Supported data sources: - Azure Blob Storage - Azure Data Lake Storage Gen2 - Microsoft OneLake - Uploaded files/folders - RAG pattern implementation## Architecture / Development ### RAG Pattern Steps 1.Retrieve: Search relevant grounding data based users prompt 2.Augment: Add retrieved data prompt 3.Generate: Use LLM produce grounded response### Adding Grounding Data Azure AI Foundry - Connect supported storage services (Blob, Data Lake, OneLake) - Upload files folders directly projects storage - Use
Use LLM produce grounded response### Adding Grounding Data Azure AI Foundry - Connect supported storage services (Blob, Data Lake, OneLake) - Upload files folders directly projects storage - Use connected data source RAG-based flows## Best Practices / Considerations - Ensure grounding data accurate, up-to-date, relevant - Use RAG LLM lacks domain-specific knowledge - Monitor quality grounded responses using evaluation metrics - Protect sensitive data uploading connecting sources## Sample Exam Questions 1.What purpose Retrieval Augmented Generation (RAG)?A. To fine-tune language model B. To generate synthetic training data C. To ground LLM responses using external data D. To monitor LLM performanceCorrect Answer:C2.Which following NOT supported data source grounding Azure AI Foundry?A. Azure Blob Storage B. Microsoft OneLake C. Google Drive D. Azure Data Lake Storage Gen2Correct Answer:C3.What three main steps RAG pattern?A. Train, Test, Deploy B. Retrieve, Augment, Generate C. Input,
Blob Storage B. Microsoft OneLake C. Google Drive D. Azure Data Lake Storage Gen2Correct Answer:C3.What three main steps RAG pattern?A. Train, Test, Deploy B. Retrieve, Augment, Generate C. Input, Process, Output D. Connect, Monitor, EvaluateCorrect Answer:B# Make Your Data Searchable Azure AI Search## Problem You want LLM-based agent retrieve accurate relevant information data. How make data searchable integrate chat flow?## Solution Azure UseAzure AI SearchintegratedAzure AI Foundryindex data enable efficient retrieval using keyword, semantic, vector, hybrid search. This allows LLM application retrieve relevant context generate grounded responses.## Required Components - Azure AI Foundry project - Azure AI Search - Data source (e.g., documents, files) - Embedding model (e.g., Azure OpenAI) - Search index (text-based vector-based)## Architecture / Development ### Creating Search Index - Upload connect data Azure AI Foundry - Use Azure AI Search create index - Choose indexing method:
- Search index (text-based vector-based)## Architecture / Development ### Creating Search Index - Upload connect data Azure AI Foundry - Use Azure AI Search create index - Choose indexing method: -Keyword search: Exact term matching -Semantic search: Meaning-based retrieval -Vector search: Embedding-based similarity -Hybrid search: Combines keyword + vector + optional semantic ranking### Using Vector Indexes -Embeddings: Represent text vectors floating-point numbers -Cosine similarity: Measures semantic similarity query documents -Embedding model: Use Azure OpenAI generate embeddings indexing -Benefits: - Retrieve semantically similar content - Support multilingual multimodal data - Improve relevance generative AI applications### Querying Index -Keyword search: Matches exact terms -Semantic search: Matches meaning -Vector search: Matches vector similarity -Hybrid search: Combines best accuracy flexibility## Best Practices / Considerations - Usevector searchsemantic-rich applications -
search: Matches meaning -Vector search: Matches vector similarity -Hybrid search: Combines best accuracy flexibility## Best Practices / Considerations - Usevector searchsemantic-rich applications - Usehybrid searchoptimal balance precision relevance - Choose right embedding model data type - Regularly update index data changes## Sample Exam Questions 1.What purpose using vector embeddings Azure AI Search?A. To compress data B. To store documents C. To enable semantic similarity search D. To encrypt search queriesCorrect Answer:C2.Which search method combines keyword, vector, semantic ranking?A. Semantic search B. Hybrid search C. Full-text search D. Indexed searchCorrect Answer:B3.What cosine similarity used vector search?A. To sort documents alphabetically B. To measure angle keyword matches C. To calculate semantic similarity vectors D. To encrypt vector dataCorrect Answer:C# Create RAG-based client applicationWhen created Azure AI Search index contextual data, use OpenAI model. To
C. To calculate semantic similarity vectors D. To encrypt vector dataCorrect Answer:C# Create RAG-based client applicationWhen created Azure AI Search index contextual data, use OpenAI model. To ground prompts data index, Azure OpenAI SDK supports extending request connection details index. The pattern using approach working Azure AI Foundry project shown following diagram.1. Use Azure AI Foundry project client retrieve connection details Azure AI Search index OpenAI ChatClient object.2. Add index connection information ChatClient configuration searched grounding data based user prompt.3. Submit grounded prompt Azure OpenAI model generate contextualized response.The following code example shows implement pattern.The following code example shows implement pattern.```pythonfrom openai import AzureOpenAI# Get Azure OpenAI chat clientchat_client = AzureOpenAI( api_version = "2024-12-01-preview", azure_endpoint = open_ai_endpoint, api_key = open_ai_key)# Initialize prompt system
import AzureOpenAI# Get Azure OpenAI chat clientchat_client = AzureOpenAI( api_version = "2024-12-01-preview", azure_endpoint = open_ai_endpoint, api_key = open_ai_key)# Initialize prompt system messageprompt = [ {"role": "system", "content": "You helpful AI assistant."}]# Add user input message promptinput_text = input("Enter question: ")prompt.append({"role": "user", "content": input_text})# Additional parameters apply RAG pattern using AI Search indexrag_params = { "data_sources": [ { "type": "azure_search", "parameters": { "endpoint": search_url, "index_name": "index_name", "authentication": { "type": "api_key", "key": search_key, } } } ],}# Submit prompt index informationresponse = chat_client.chat.completions.create( model="<model_deployment_name
