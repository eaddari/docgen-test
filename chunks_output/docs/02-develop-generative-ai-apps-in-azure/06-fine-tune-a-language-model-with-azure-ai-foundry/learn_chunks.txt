[H1] Understand When Fine-Tune Language Model## Problem You want improve quality consistency responses generated language model chat application. How decide whether use prompt engineering, RAG, fine-tuning?## Solution Azure Useprompt engineeringquick improvements,RAGground responses factual data,fine-tuningneed consistent behavior, tone, formatting prompt engineering alone cannot guarantee.## Required Components - Azure AI Foundry project - Prompt Flow - Base language model - Training dataset (for fine-tuning) - Optional: Azure AI Search (for RAG)## Architecture / Development ### Optimization Strategies 1.Prompt Engineering- Modify prompt structure system message - Use one-shot few-shot examples - Fast easy, may lack consistency2.Retrieval Augmented Generation (RAG)- Retrieve relevant context external data - Ground responses factual, domain-specific information - Ideal knowledge-based applications3.Fine-Tuning- Train base model dataset - Achieve consistent tone, style, format - Useful
external data - Ground responses factual, domain-specific information - Ideal knowledge-based applications3.Fine-Tuning- Train base model dataset - Achieve consistent tone, style, format - Useful prompt engineering insufficient### When Use Fine-Tuning - You need consistent output style tone - Prompt engineering fails enforce structure - You want model behave specific way across interactions - You labeled dataset training## Best Practices / Considerations - Start prompt engineering quick wins - Use RAG factual accuracy critical - Fine-tune necessary due higher complexity cost - Combine strategies (e.g., RAG + fine-tuned model) best results## Sample Exam Questions 1.What primary goal fine-tuning language model?A. To reduce latency B. To improve factual accuracy C. To enforce consistent behavior style D. To increase token limitsCorrect Answer:C2.Which technique best grounding model external data?A. Prompt engineering B. Fine-tuning C. Retrieval Augmented Generation (RAG) D. Few-shot
behavior style D. To increase token limitsCorrect Answer:C2.Which technique best grounding model external data?A. Prompt engineering B. Fine-tuning C. Retrieval Augmented Generation (RAG) D. Few-shot learningCorrect Answer:C3.What limitation prompt engineering?A. It requires labeled training data B. It cannot modify system messages C. It may consistently enforce output format D. It supported Azure AI FoundryCorrect Answer:C# Prepare data fine-tune chat completion modelFine-tuning involves combining suitable foundation model use base, set training data includes example prompts responses model learn from.Diagram base model plus training data resulting fine-tuned model.When decide want fine-tune language model, need identify dataset use fine-tune language model.Similar machine learning model, quality dataset large effect quality model. Though need less data would train language model scratch, still might need enough data maximize consistency desired model's behavior. How much data need
quality dataset large effect quality model. Though need less data would train language model scratch, still might need enough data maximize consistency desired model's behavior. How much data need depends use case.When fine-tune language model chat completion, data use fine-tune model collection sample conversations. More specifically, data contain three components:- The system message- The user message- The assistant's responseThe three variables come together JSON Lines JSONL file. For example, one line dataset might look like:json{"messages": [{"role": "system", "content": "You Xbox customer support agent whose primary goal help users issues experiencing Xbox devices. You friendly concise. You provide factual answers queries, provide answers related Xbox."}, {"role": "user", "content": "Is Xbox better PlayStation?"}, {"role": "assistant", "content": "I apologize, I cannot provide personal opinions. My primary job assist issues related Xbox device. Do Xbox-related issues need
"Is Xbox better PlayStation?"}, {"role": "assistant", "content": "I apologize, I cannot provide personal opinions. My primary job assist issues related Xbox device. Do Xbox-related issues need addressing?"}]}The dataset show model's ideal behavior. You create dataset based chat history chat application have. A things keep mind use real data to:Remove personal sensitive information.Not focus creating large training dataset, also ensure dataset includes diverse set examples.You include multiple turns conversation single line dataset. If want fine-tune specific assistant messages, optionally use weight key-value pair. When weight set 0, message ignored, set 1, message included training.An example multi-turn chat file format weights:json{"messages": [{"role": "system", "content": "Marv factual chatbot also sarcastic."}, {"role": "user", "content": "What's capital France?"}, {"role": "assistant", "content": "Paris", "weight": 0}, {"role": "user", "content": "Can sarcastic?"}, {"role":
chatbot also sarcastic."}, {"role": "user", "content": "What's capital France?"}, {"role": "assistant", "content": "Paris", "weight": 0}, {"role": "user", "content": "Can sarcastic?"}, {"role": "assistant", "content": "Paris, everyone know already.", "weight": 1}]}When preparing dataset fine-tune language model, understand desired model behaviors, create dataset JSONL format, ensure examples include high quality diverse. By preparing dataset, higher chance fine-tuned model improves chat application's performance.# Explore Fine-Tuning Language Models Azure AI Studio## Problem You want customize language model better suit specific task (e.g., chat completion, classification, translation). How fine-tune foundation model using Azure AI Studio?## Solution Azure UseAzure AI Studioselect base model model catalog fine-tune dataset. You configure fine-tuning job portal deploy resulting model use application.## Required Components - Azure AI Foundry project - Access Azure AI Studio - Foundation
model catalog fine-tune dataset. You configure fine-tuning job portal deploy resulting model use application.## Required Components - Azure AI Foundry project - Access Azure AI Studio - Foundation model (e.g., GPT-4, Llama-2-7b) - Training dataset (and optional validation dataset) - Fine-tuning configuration parameters## Architecture / Development ### 1. Select Base Model - NavigateModel CatalogAzure AI Studio - Filter models fine-tuning task (e.g., chat completion) - Consider: -Model capabilities(e.g., BERT short texts) -Pretraining data(e.g., GPT-2 trained internet data) -Biases limitations-Language support- Review model cards (linked Hugging Face) detailed info### 2. Configure Fine-Tuning Job Steps:1. Select base model 2. Upload/select training data 3. (Optional) Upload validation data 4. Configure advanced options:| Parameter | Description ||------------------------|-------------||batch_size| Number examples per training step. Larger sizes = less frequent updates, lower variance.
4. Configure advanced options:| Parameter | Description ||------------------------|-------------||batch_size| Number examples per training step. Larger sizes = less frequent updates, lower variance. ||learning_rate_multiplier| Multiplies base learning rate. Try values 0.02 0.2. ||n_epochs| Number full passes training data. ||seed| Controls reproducibility training results. |- Submit job monitor status portal - Review input parameters validation results completion### 3. Deploy Test - Deploy fine-tuned model endpoint - Test models performance - Integrate chat application satisfied## Best Practices / Considerations - Ensure model available region quota - Use validation data evaluate performance - Start prompt engineering RAG fine-tuning - Fine-tune consistent behavior task-specific adaptation required## Sample Exam Questions 1.Which parameter controls many training examples used per step?A.n_epochsB.learning_rate_multiplierC.batch_sizeD.seedCorrect Answer:C2.Where find detailed
adaptation required## Sample Exam Questions 1.Which parameter controls many training examples used per step?A.n_epochsB.learning_rate_multiplierC.batch_sizeD.seedCorrect Answer:C2.Where find detailed information foundation model Azure AI Studio?A. Azure CLI B. Model card linked catalog C. Azure Monitor D. Azure Key VaultCorrect Answer:B3.What purposelearning_rate_multiplier?A. To increase number epochs B. To control size training dataset C. To scale base learning rate fine-tuning D. To set models output formatCorrect Answer:C
