[H1] Custom Named Entity Recognition (Custom NER) (Azure AI Language Service)## ProblemYou need extract specific entities (user-defined) unstructured text built-in NER insufficient unavailable. Entities may include legal terms, financial fields, domain-specific information.## Solution AzureUseAzure AI Language - Custom Named Entity Recognition (Custom NER)to:- Define entities- Label train model- Deploy integrate via API## Components Required- Azure AI Language resource (Custom NER feature enabled)- Azure Storage account (blob storage)- Labeled dataset (training + testing)- Language Studio labeling training- Azure REST API deployment extraction- Role assignment: Storage Blob Data Contributor storage access needed## Architecture / Development### 1 Custom vs Built-in NER| Feature | Built-in NER | Custom NER ||---------|--------------|------------|| Entity Types | Predefined (person, location, organization, URL, etc.) | User-defined || Configuration | Minimal | Full training cycle || Use
NER | Custom NER ||---------|--------------|------------|| Entity Types | Predefined (person, location, organization, URL, etc.) | User-defined || Configuration | Minimal | Full training cycle || Use Cases | Generic extraction | Domain-specific extraction |### 2 Custom NER Project Lifecycle#### Define Entities- Clearly define entity want extract- Avoid ambiguity- Split complex fields (e.g. contact info phone, email, social)#### Tag Data (Labeling)- Use Language Studio select tag text fragments entities#### Train Model- After labeling, train model#### Evaluate ModelMetrics used:-Precision(Correct labels / total predictions)-Recall(Correct labels / total actual entities)-F1 Score(Harmonic mean precision & recall)#### Improve ModelAnalyze underperforming entities via:- Model metrics- Confusion matrix#### Deploy Model- After acceptable performance, deploy production use#### Extract Entities (Inference)- Use deployed model via API extract entities### 3 Example API Request
metrics- Confusion matrix#### Deploy Model- After acceptable performance, deploy production use#### Extract Entities (Inference)- Use deployed model via API extract entities### 3 Example API Request (CustomEntityRecognition)json{ "displayName": "string", "analysisInput": { "documents": [ { "id": "doc1", "text": "string" }, { "id": "doc2", "text": "string" } ] }, "tasks": [ { "kind": "CustomEntityRecognition", "taskName": "MyRecognitionTaskName", "parameters": { "projectName": "MyProject", "deploymentName": "MyDeployment" } } ]}### 4 Accepted Training Data Format (JSON schema example)json{ "projectFileVersion": "{DATE}", "stringIndexType": "Utf16CodeUnit", "metadata": { "projectKind": "CustomEntityRecognition", "storageInputContainerName": "{CONTAINER-NAME}", "projectName": "{PROJECT-NAME}", "language": "en-us" }, "assets": { "entities": [ { "category": "Entity1" }, { "category": "Entity2" } ], "documents": [ { "location": "{DOCUMENT-NAME}", "language": "{LANGUAGE-CODE}", "dataset":
"language": "en-us" }, "assets": { "entities": [ { "category": "Entity1" }, { "category": "Entity2" } ], "documents": [ { "location": "{DOCUMENT-NAME}", "language": "{LANGUAGE-CODE}", "dataset": "{DATASET}", "entities": [ { "regionOffset": 0, "regionLength": 500, "labels": [ { "category": "Entity1", "offset": 25, "length": 10 }, { "category": "Entity2", "offset": 120, "length": 8 } ] } ] } ] }}### 5 Model Evaluation Scenarios| Precision | Recall | Interpretation ||-----------|--------|----------------|| Low | Low | Model struggles recognition labeling || High | Low | Correct labels missing entities || Low | High | Finds entities often assigns wrong labels |### 6 Confusion Matrix- Visual table predicted vs actual entities- Identifies entities need training data### 7 Project Limits (Service Quotas)| Resource | Limit ||----------|-------|| Training files | 10 - 100,000 || Deployments | 10 per project || Authoring API limits | 10 POST / 100 GET per min || Analyze API limits | 20 GET POST
Quotas)| Resource | Limit ||----------|-------|| Training files | 10 - 100,000 || Deployments | 10 per project || Authoring API limits | 10 POST / 100 GET per min || Analyze API limits | 20 GET POST || Projects | 500 per resource || Models | 50 trained models per project || Entity types | 200 || Entity character length | 500 |## Best Practice / Considerations- Use high-quality, diverse, real-world-like data training- Label consistency, precision, completeness- Avoid ambiguous entity definitions- Separate compound entities distinct categories- Use confusion matrix drive iterative improvement- Secure storage accounts properly production- Monitor quota limits scaling## Exam-like Sample Questions### Question 1:Which task type specified Custom NER submitting request?A. CustomTextClassification B. CustomEntityRecognition C. EntityDetectionAnswer: B### Question 2:Which metric indicates model correctly labels entities finds?A. Precision B. Recall C. F1 ScoreAnswer: A### Question 3:What
B. CustomEntityRecognition C. EntityDetectionAnswer: B### Question 2:Which metric indicates model correctly labels entities finds?A. Precision B. Recall C. F1 ScoreAnswer: A### Question 3:What maximum number entity types allowed Custom NER project?A. 100 B. 200 C. 500Answer: B### Question 4:What tool use visually identify misclassified entities model evaluation?A. Model Matrix B. Confusion Matrix C. Precision ReportAnswer: B### Question 5:Which labeling practice improves model accuracy?A. Label obvious entities B. Use synthetic data exclusively C. Maintain precision, consistency, completenessAnswer: C
