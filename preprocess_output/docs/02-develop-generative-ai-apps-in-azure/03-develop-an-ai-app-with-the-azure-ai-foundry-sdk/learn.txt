[H1] What Azure AI Foundry SDK?The Azure AI Foundry SDK set packages services designed work together enable developers write code uses resources Azure AI Foundry project. With Azure AI Foundry SDK, developers create applications connect project, access resource connections models project, use perform AI operations, sending prompts generative AI model processing responsesThe SDK provides Python Microsoft C# .NET libraries use build AI applications based Azure AI Foundry projects.## Installing SDK packagesThe core package working projects Azure AI Foundry SDK Azure AI Projects library, enables connect Azure AI Foundry project access resources defined within it.To use Azure AI Projects library Python, use pip package installation utility install azure-ai-projects package PyPi:bashpip install azure-ai-projects## Using SDK connect projectThe first task Azure AI Foundry SDK code connect Azure AI Foundry project. Each project unique endpoint, find project's Overview page Azure AI Foundry portal.### Note: The project provides multiple endpoints keys, including:- An endpoint project itself; used access project connections, agents, models Azure AI Foundry resource.- An endpoint Azure OpenAI Service APIs project's Azure AI Foundry resource.- An endpoint Azure AI services APIs (such Azure AI Vision Azure AI Language) Azure AI Foundry resource.You use project endpoint code create AIProjectClient object, provides programmatic proxy project.The following code snippet shows create AIProjectClient object Python.pythonfrom azure.identity import DefaultAzureCredentialfrom azure.ai.projects import AIProjectClient...project_endpoint = " = AIProjectClient( credential=DefaultAzureCredential(), endpoint=project_endpoint)### Note: The code uses default Azure credentials authenticate accessing project. To enable authentication, addition azure-ai-projects package, need install azure-identity package:bashpip install azure-identity### Tip: To access project successfully, code must run context authenticated Azure session. For example, could use Azure command-line interface (CLI) az-login command sign running code.# Work project connectionsEach Azure AI Foundry project includes connected resources, defined parent (Azure AI Foundry resource hub) level, project level. Each resource connection external service, Azure storage, Azure AI Search, Azure OpenAI, another Azure AI Foundry resource.With Azure AI Foundry SDK, connect project retrieve connections; use consume connected services.The AIProjectClient object Python connections property, use access resource connections project. Methods connections object include:connections.list(): Returns collection connection objects, representing connection project. You filter results specifying optional connection_type parameter valid enumeration, ConnectionType.AZURE_OPEN_AI.connections.get(connection_name, include_credentials): Returns connection object connection name specified. If include_credentials parameter True (the default value), credentials required connect connection returned - example, form API key Azure AI services resource.The connection objects returned methods include connection-specific properties, including credentials, use connect associated resource.The following code example lists resource connections added project:pythonfrom azure.identity import DefaultAzureCredentialfrom azure.ai.projects import AIProjectClienttry: # Get project client project_endpoint = " project_client = AIProjectClient( credential=DefaultAzureCredential(), endpoint=project_endpoint, ) ## List connections project connections = project_client.connections print("List connections:") connection connections.list(): print(f"{connection.name} ({connection.type})")except Exception ex: print(ex)# Create chat clientA common scenario AI application connect generative AI model use prompts engage chat-based dialog it. You use Azure AI Foundry SDK chat models deployed Azure AI Foundry project.The specific libraries code used build chat client depends target model deployed Azure AI Foundry project. You deploy models following model hosting solutions:- Azure AI Foundry Models: A single endpoint multiple models different types, including OpenAI models others Azure AI Foundry model catalog. Models consumed Azure AI Foundry resource connection project (either default Azure AI Foundry resource project another resource connection added project).- Azure OpenAI: A single endpoint OpenAI models hosted Azure. Models consumed Azure OpenAI resource connection project.- Serverless API: A model-as-a-service solution deployed model accessed unique endpoint hosted Azure AI Foundry project.- Managed compute: A model-as-a-service solution deployed model accessed unique endpoint hosted custom compute.### Note: To deploy models Azure AI model inference endpoint, must enable Deploy models Azure AI model inference service option Azure AI Foundry.In module, focus models deployed Azure AI Foundry Models endpoint.## Building client app Azure AI Foundry ModelsWhen deployed models Azure AI model inference service, use Azure AI Foundry SDK write code creates ChatCompletionsClient object, use chat deployed model. One benefits using model deployment type easily switch deployed models changing one parameter code (the model deployment name), making great way test multiple models developing app.The following Python code sample uses ChatCompletionsClient object chat model deployment named phi-4-model.pythonfrom azure.ai.projects import AIProjectClientfrom azure.identity import DefaultAzureCredentialfrom azure.ai.inference.models import SystemMessage, UserMessagetry: # Initialize project client project_client = AIProjectClient( credential=DefaultAzureCredential(), endpoint=project_endpoint) ## Get chat client chat_client = project_client.inference.get_chat_completions_client() # Get chat completion based user-provided prompt user_prompt = input("Enter question:") response = chat_client.complete( model="phi-4-model", messages=[ SystemMessage("You helpful AI assistant answers questions."), UserMessage(user_prompt) ], ) print(response.choices[0].message.content)except Exception ex: print(ex)### Note: The ChatCompletionsClient class uses Azure AI Inference library. In addition azure-ai-projects azure-identity packages discussed previously, sample code shown assumes azure-ai-inference package installed:bashpip install azure-ai-inference## Using Azure OpenAI SDKIn Azure AI Foundry SDK Python, AIProjectClient class provides get_azure_openai_client() method use create Azure OpenAI client object. You use classes methods defined Azure OpenAI SDK consume OpenAI model deployed Azure Foundry Models.The following Python code sample uses Azure AI Foundry Azure OpenAI SDKs chat model deployment named gpt-4o-model.pythonfrom azure.identity import DefaultAzureCredentialfrom azure.ai.projects import AIProjectClientfrom openai import AzureOpenAItry: # Initialize project client project_connection_string = " project_client = AIProjectClient( credential=DefaultAzureCredential(), endpoint=project_endpoint) ## Get Azure OpenAI chat client openai_client = project_client.inference.get_azure_openai_client(api_version="2024-10-21") # Get chat completion based user-provided prompt user_prompt = input("Enter question:") response = openai_client.chat.completions.create( model="gpt-4o-model", messages=[ {"role": "system", "content": "You helpful AI assistant answers questions."}, {"role": "user", "content": user_prompt}, ] ) print(response.choices[0].message.content)except Exception ex: print(ex)### Note: In addition azure-ai-projects azure-identity packages discussed previously, sample code shown assumes openai package installed:bashpip install openai