[H2] Assess Model PerformanceProblemHow evaluate effectiveness, quality, safety language model generative AI application development deployment?Solution AzureUse combination model benchmarks, manual evaluations, AI-assisted metrics, NLP metrics assess model performance. Evaluate individual models complete chat flows ensure reliability alignment user expectations.Required Components- Azure AI Foundry portal (for model benchmarks)- Human evaluators (for manual assessments)- AI-assisted evaluation tools- NLP metric tools (e.g., BLEU, ROUGE, METEOR)Architecture / Development1.Model Benchmarks- Use public benchmarks compare models. - Common metrics: -Accuracy: Exact match expected output. -Coherence: Logical flow natural language. -Fluency: Grammatical correctness vocabulary use. -GPT Similarity: Semantic similarity ground truth.2.Manual Evaluations- Human raters assess responses for: - Relevance - Informativeness - Engagement - Useful capturing subjective quality aspects.3.AI-Assisted Metrics-Generation Quality: Creativity, coherence, tone. -Risk Safety: Detection harmful biased content.4.Natural Language Processing (NLP) Metrics-F1-score: Shared word ratio generated ground truth. -BLEU: Measures translation quality. -METEOR: Considers synonymy word order. -ROUGE: Measures recall overlapping units.5.Evaluation Scope- Evaluate both: - Individual model responses - Full chat flows multiple nodes logicBest Practices / Considerations- Start individual model testing, expand full app evaluation.- Combine manual automated methods comprehensive insights.- Use Azure AI Foundry explore compare model benchmarks.- Continuously monitor refine evaluation criteria.Sample Exam Questions1. What key differences manual AI-assisted evaluations?2. Which metrics commonly used assess fluency coherence model outputs?3. How F1-score help evaluate model performance?4. What purpose using model benchmarks Azure AI Foundry?## Manually Evaluate Performance ModelProblemHow assess quality relevance language models responses development deployment?Solution AzureUse Azure AI Foundry portal manually evaluate models prompt flows. This includes testing individual prompts chat playground evaluating multiple prompts using uploaded datasets.Required Components- Azure AI Foundry portal- Chat playground- Manual evaluations feature- Test prompt dataset (with optional expected responses)Architecture / Development1.Prepare Test Prompts- Create diverse set prompts covering typical use cases, edge cases, failure scenarios.2.Test Chat Playground- Interact model directly. - Modify prompts system messages observe changes output. - Use quick iteration early development.3.Evaluate Multiple Prompts- Use manual evaluations feature upload dataset prompts. - Optionally include expected responses comparison. - Rate responses using thumbs up/down. - Use feedback adjust prompts, system messages, model parameters.4.Evaluate Prompt Flows- After validating individual models, integrate prompt flows. - Evaluate entire flows manually automatically ensure end-to-end performance.Best Practices / Considerations- Use manual evaluations early throughout development lifecycle.- Include wide range prompt types uncover weaknesses.- Combine manual insights automated metrics holistic view.- Continuously refine prompts flows based evaluation results.Sample Exam Questions1. What purpose chat playground Azure AI Foundry?2. How evaluate multiple prompts efficiently Azure AI Foundry?3. Why manual evaluations important even deploying model?4. What types feedback collected manual evaluation?## Automated EvaluationsProblemHow efficiently consistently assess quality safety model outputs scale?Solution AzureUse automated evaluations Azure AI Foundry portal evaluate models, datasets, prompt flows using predefined metrics AI-based evaluators.Required Components- Dataset prompts responses (with optional ground truth)- Azure AI Foundry portal- Evaluation metrics evaluators (AI Quality, Risk Safety)Architecture / Development1.Evaluation Data- Prepare dataset prompts responses. - Optionally include expected responses ground truth. - You generate initial data using AI model refine manually.2.Evaluation Metrics- Choose evaluators based evaluation goals: -AI Quality: - Coherence - Relevance - NLP metrics: F1 score, BLEU, METEOR, ROUGE -Risk Safety: - Detection content related violence, hate, sexual content, self-harm3.Execution- Run evaluations Azure AI Foundry portal. - Analyze results identify areas improvement model performance safety.Best Practices / Considerations- Use AI-generated data bootstrap evaluation datasets.- Combine automated evaluations manual reviews comprehensive insights.- Regularly update evaluation datasets metrics reflect evolving use cases risks.Sample Exam Questions1. What types metrics used automated evaluations Azure AI Foundry?2. How generate evaluation data automated testing?3. What two main categories evaluators available automated evaluations?4. Why beneficial include ground truth responses evaluation dataset?