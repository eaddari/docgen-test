[H1] Evaluate generative AI model performanceIn exercise, youll use manual automated evaluations assess performance model Azure AI Foundry portal.## Create Azure AI Foundry hub projectThe features Azure AI Foundry going use exercise require project based Azure AI Foundry hub resource.1. In web browser, open Azure AI Foundry portal sign using Azure credentials. Close tips quick start panes opened first time sign in, necessary use Azure AI Foundry logo top left navigate home page, looks similar following image (close Help pane open):2. In browser, navigate select Create. Then choose option create new AI hub resource.3. In Create project wizard, enter valid name project, select option create new hub. Then use Rename hub link specify valid name new hub, expand Advanced options, specify following settings project: - Subscription: Your Azure subscription - Resource group: Create select resource group - Region: Select one following locations (In event quota limit exceeded later exercise, may need create another resource different region.): - East US 2 - France Central - UK South - Sweden Central### Note: If youre working Azure subscription policies used restrict allowable resource names, may need use link bottom Create new project dialog box create hub using Azure portal.### Tip: If Create button still disabled, sure rename hub unique alphanumeric value.Wait project created.## Deploy modelsIn exercise, youll evaluate performance gpt-4o-mini model. Youll also use gpt-4o model generate AI-assisted evaluation metrics.1. In navigation pane left project, My assets section, select Models + endpoints page.2. In Models + endpoints page, Model deployments tab, + Deploy model menu, select Deploy base model.3. Search gpt-4o model list, select confirm it.4. Deploy model following settings selecting Customize deployment details: - Deployment name: A valid name model deployment - Deployment type: Global Standard - Automatic version update: Enabled - Model version: Select recent available version - Connected AI resource: Select Azure OpenAI resource connection - Tokens per Minute Rate Limit (thousands): 50K (or maximum available subscription less 50K) - Content filter: DefaultV2### Note: Reducing TPM helps avoid over-using quota available subscription using. 50,000 TPM sufficient data used exercise. If available quota lower this, able complete exercise may experience errors rate limit exceeded.5. Wait deployment complete.6. Return Models + endpoints page repeat previous steps deploy gpt-4o-mini model settings.## Manually evaluate modelYou manually review model responses based test data. Manually reviewing allows test different inputs evaluate whether model performs expected.1. In new browser tab, download travel_evaluation_data.jsonl save local folder travel_evaluation_data.jsonl (be sure save .jsonl file, .txt file).2. Back Azure AI Foundry portal tab, navigation pane, Protect govern section, select Evaluation.3. If Create new evaluation pane opens automatically, select Cancel close it.4. In Evaluation page, view Manual evaluations tab select + New manual evaluation.5. In Configurations section, Model list, select gpt-4o model deployment.6. Change System message following instructions AI travel assistant:codeAssist users travel-related inquiries, offering tips, advice, recommendations knowledgeable travel agent.7. In Manual evaluation result section, select Import test data upload travel_evaluation_data.jsonl file downloaded previously; scrolling map dataset fields follows: - Input: Question - Expected response: ExpectedResponse8. Review questions expected answers test file - youll use evaluate responses model generates.9. Select Run top bar generate outputs questions added inputs. After minutes, responses model shown new Output column, like this:10. Review outputs question, comparing output model expected answer scoring results selecting thumbs icon bottom right response.11. After youve scored responses, review summary tiles list. Then toolbar, select Save results assign suitable name. Saving results enables retrieve later evaluation comparison different model.## Use automated evaluationWhile manually comparing model output expected responses useful way assess models performance, time-consuming approach scenarios expect wide range questions responses; provides little way standardized metrics use compare different model prompt combinations.Automated evaluation approach attempts address shortcomings calculating metrics using AI assess responses coherence, relevance, factors.1. Use back arrow () next Manual evaluation page title return Evaluation page.2. View Automated evaluations tab.3. Select Create new evaluation, prompted, select option evaluate Evaluate model select Next.4. On Select data source page, select Use dataset select travel_evaluation_data_jsonl_xxxx dataset based file uploaded previously, select Next.5. On Test model page, select gpt-4o-mini model change System message instructions AI travel assistant used previously:codeAssist users travel-related inquiries, offering tips, advice, recommendations knowledgeable travel agent.6. For query field, select {{item.question}}.7. Select Next move next page.8. On Configure evaluators page, use +Add button add following evaluators, configuring one follows:- Model scorer: - Criteria name: Semantic_similarity - Grade with: Select gpt-4o model - User settings (at bottom): Output: {{sample.output_text}} Ground Truth: {{item.ExpectedResponse}}- Likert-scale evaluator: - Criteria name: Relevance Grade with: Select gpt-4o model Query: {{item.question}}- Text similarity: - Criteria name: F1_Score Ground truth: {{item.ExpectedResponse}}- Hateful unfair content: - Criteria name: Hate_and_unfairness Query: {{item.question}}9. Select Next review evaluation settings. You configured evaluation use travel evaluation dataset evaluate gpt-4o-mini model semantic similarity, relevance, F1 score, hateful unfair language.10. Give evaluation suitable name, Submit start evaluation process, wait complete. It may take minutes. You use Refresh toolbar button check status.11. When evaluation completed, scroll necessary review results.12. At top page, select Data tab see raw data evaluation. The data includes metrics input well explanations reasoning gpt-4o model applied assessing responses.## Clean upWhen finish exploring Azure AI Foundry, delete resources youve created avoid unnecessary Azure costs.- Navigate Azure portal In Azure portal, Home page, select Resource groups.- Select resource group created exercise.- At top Overview page resource group, select Delete resource group.- Enter resource group name confirm want delete it, select Delete.