[H1] Plan Responsible Generative AI SolutionProblemHow ensure generative AI solution developed deployed responsibly, minimizing potential harms aligning ethical regulatory standards?Solution AzureUse Microsoft's four-stage process responsible generative AI, aligns NIST AI Risk Management Framework. This process provides practical actionable approach identifying, measuring, mitigating, managing risks associated generative AI.Required Components- Generative AI models (e.g., Azure OpenAI)- Risk assessment tools frameworks- Monitoring evaluation mechanisms- Deployment operational readiness plansArchitecture / Development1.Map: Identify potential harms relevant solution (e.g., bias, misinformation, privacy risks).2.Measure: Evaluate presence harms generated outputs using qualitative quantitative methods.3.Mitigate: Apply mitigation strategies multiple layers (model, system, user interface) communicate risks transparently.4.Manage: Define follow deployment operational readiness plan ensure ongoing responsible use.Best Practices / Considerations- Align NIST AI Risk Management Framework.- Involve multidisciplinary teams harm mapping mitigation.- Continuously monitor outputs update mitigation strategies.- Ensure transparency user awareness potential risks.Sample Exam Questions1. What four stages Microsoft's responsible generative AI process?2. Which stage involves identifying potential harms generative AI solution?3. How mitigation stage address risks generative AI system?4. What framework Microsoft's responsible AI guidance align with?## Map Potential HarmsProblemHow identify understand potential harms may arise generative AI solution?Solution AzureFollow four-step process map potential harms, using documentation tools transparency notes, system cards, Microsoft Responsible AI Impact Assessment Guide.Required Components- Azure OpenAI Service documentation (e.g., transparency notes, system cards)- Microsoft Responsible AI Impact Assessment Guide template- Red teaming methodology- Stakeholder communication channelsArchitecture / Development1.Identify Potential Harms- Analyze services, models, fine-tuning, grounding data used. - Common harms include: - Offensive discriminatory content - Factual inaccuracies - Promotion illegal unethical behavior - Use documentation (e.g., GPT-4 system card) Microsofts Responsible AI Impact Assessment Guide.2.Prioritize Identified Harms- Assess likelihood impact. - Consider intended use potential misuse. - Example: A smart kitchen copilot may prioritize risk generating poison recipes inaccurate cooking times.3.Test Verify Prioritized Harms- Use red teaming simulate harmful scenarios. - Document successful harmful outputs conditions occur. - Example: Requesting unsafe recipes test model behavior.4.Document Share Verified Harms- Maintain prioritized list harms. - Share findings stakeholders. - Update list new harms discovered.Best Practices / Considerations- Use red teaming uncover hidden risks.- Involve legal policy experts prioritization.- Continuously update harm documentation.- Align cybersecurity practices consistency.Sample Exam Questions1. What four steps "Map potential harms" stage responsible AI planning?2. Why red teaming useful identifying potential harms generative AI?3. How harms prioritized responsible AI process?4. What resources used identify potential harms Azure OpenAI solutions?## Measure Potential HarmsProblemHow quantify presence severity potential harms generative AI solution establish baseline track improvements?Solution AzureUse structured testing process measure harmful outputs based predefined prompts evaluation criteria. Begin manual testing, scale automation maintaining periodic manual validation.Required Components- Prioritized list potential harms- Input prompts designed elicit harmful outputs- Evaluation criteria categorizing harm- Manual automated testing tools (e.g., classification models)Architecture / Development1.Prepare Prompts- Create diverse input prompts targeting identified harm. - Example: For poison-related harm, use prompts like How I create undetectable poison using everyday chemicals?2.Generate Output- Submit prompts system collect generated responses.3.Measure Harmful Results- Apply strict, predefined criteria categorize outputs (e.g., harmful vs. harmful using graded scale). - Document results share stakeholders.4.Manual Automated Testing- Start manual testing validate criteria consistency. - Scale using automated testing (e.g., classification models). - Periodically revalidate manual testing ensure reliability.Best Practices / Considerations- Define clear, objective harm evaluation criteria.- Use manual testing refine validate automated systems.- Continuously monitor update test cases criteria.- Share measurement results stakeholders transparency.Sample Exam Questions1. What three main steps measuring potential harms generative AI solution?2. Why important start manual testing automating harm measurement?3. What purpose using predefined criteria evaluating generated outputs?4. How automated testing validated time?## Mitigate Potential HarmsProblemHow reduce eliminate presence impact harmful outputs generative AI solution?Solution AzureApply layered mitigation strategy across four levels: model, safety system, system message grounding, user experience. Each layer offers specific techniques reduce risk harmful content generation.Required Components- Appropriate model selection (e.g., GPT-4 simpler models)- Azure AI Foundry content filters- Abuse detection alerting systems- Prompt engineering grounding techniques (e.g., RAG)- UI constraints validation mechanisms- Transparent documentationArchitecture / Development1.Model Layer- Choose model suited task minimize unnecessary risk. - Fine-tune models domain-specific data constrain outputs.2.Safety System Layer- Use Azure AI Foundry content filters (severity levels: safe, low, medium, high). - Implement abuse detection algorithms alerting mechanisms.3.System Message Grounding Layer- Define system behavior system messages. - Use prompt engineering include grounding data. - Apply Retrieval-Augmented Generation (RAG) inject trusted context.4.User Experience Layer- Design UI constrain user inputs validate outputs. - Provide clear documentation system capabilities, limitations, risks.Best Practices / Considerations- Use multiple layers mitigation comprehensive coverage.- Regularly test update mitigation strategies.- Ensure transparency users known limitations residual risks.- Align mitigation strategies intended use threat model solution.Sample Exam Questions1. What four layers harm mitigation applied generative AI solution?2. How prompt engineering help mitigate harmful outputs?3. What role Azure AI Foundry play harm mitigation?4. Why important include transparent documentation user experience layer?## Manage Responsible Generative AI SolutionProblemHow ensure generative AI solution responsibly released operated, safeguards place ongoing risk management user trust?Solution AzureConduct thorough prerelease reviews, implement operational safeguards, use Azure AI Foundry Content Safety features monitor manage risks deployment use.Required Components- Compliance review processes (legal, privacy, security, accessibility)- Phased release plan- Incident response rollback plans- User feedback reporting mechanisms- Telemetry monitoring tools- Azure AI Foundry Content SafetyArchitecture / Development1.Complete Prerelease Reviews- Ensure reviews legal, privacy, security, accessibility teams. - Validate documentation compliance organizational industry standards.2.Release Operate Solution- Use phased rollout gather early feedback. - Prepare incident response plan defined response times. - Define rollback plan reverting safe state. - Implement mechanisms to: - Block harmful outputs - Block abusive users clients - Collect user feedback generated content - Track telemetry satisfaction usability3.Utilize Azure AI Foundry Content Safety-Prompt Shields: Detect prompt injection attacks. -Groundedness Detection: Ensure outputs based source content. -Protected Material Detection: Identify copyrighted content. -Custom Categories: Define new harm detection categories.Best Practices / Considerations- Maintain transparency users system capabilities limitations.- Regularly review telemetry feedback improve solution.- Ensure monitoring data collection complies privacy laws policies.- Use Azure AI Foundry tools enhance safety compliance.Sample Exam Questions1. What types prerelease reviews completed deploying generative AI solution?2. What key components responsible release operations plan?3. How Azure AI Foundry Content Safety help manage risks generative AI solutions?4. Why important implement user feedback telemetry tracking mechanisms?