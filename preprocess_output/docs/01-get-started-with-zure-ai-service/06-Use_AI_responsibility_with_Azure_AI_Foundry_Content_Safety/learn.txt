[H1] Azure AI Foundry Content Safety Guide## ProblemYou need detect moderate harmful, offensive, inappropriate contentwhether user-generated AI-generatedacross various formats (text, image, multimodal) languages, ensuring compliance regulations protecting user safety brand integrity.## Solution AzureUseAzure AI Foundry Content Safety, AI-powered content moderation service analyzes text, images, multimodal inputs detect content falls four core categories:ViolenceHate speech*Sexual contentSelf-harmIt replaces deprecated Azure Content Moderator provides advanced, multilingual multimodal content safety tools.## Required ComponentsAzure AI Foundry Portal(access via Azure portal)Content Safety StudioAzure AI Content StudioModeration APIsfor: * Text content * Image content * Multimodal content (OCR + analysis)Custom Categories(optional)Prompt Shields APILLM prompt protection**Protected Material DetectionGroundedness Detection APISafety System Messages## Architecture / Development### Text ModerationInput analyzed NLP across four categoriesSeverity levels (06) returned per categoryUseblocklistsspecific termsAPI returns structured moderation data### Image ModerationUsesFlorence foundation modelReturns severity:safe,low,highDeveloper sets threshold:low,medium,highEvaluated per category### Multimodal ModerationUses OCR extract analyze text within imagesSame four categories evaluated### Prompt ShieldsBlocks prompt-based jailbreaks LLMsApplies user input embedded document content### Protected Material DetectionFlags copyright violations AI-generated content### Groundedness DetectionCompares LLM responses source data* Optionally returnsreasoningungrounded flags### Custom CategoriesDefine categories using positive/negative examplesTrain model customized moderation## Best Practices / Considerations*Test real datadeploying* Continuouslymonitor accuracypost-deploymentUse human moderators edge cases appealsCommunicate clearly users content flagged* Understand AI limits: possibilityfalse positives/negativesEvaluate using: * True Positives * False Positives * True Negatives * False Negatives## Simulated Exam Questions1.How Azure AI Foundry Content Safety determine text blocked approved?By assigning severity levels (06) across four categories: violence, hate, sexual content, self-harm.2.What model powers image analysis Foundry Content Safety?Florence foundation model.3.How system evaluate multimodal content?It uses OCR extract text images, analyzes image text across four categories.4.Which service use prevent prompt-based jailbreaks LLM inputs?Prompt Shields.5.What deploying Foundry Content Safety live environment?Test real data plan continuous monitoring.6.Which functionality detects grounded vs. ungrounded responses LLMs?Groundedness Detection.7.Can define custom moderation rules? How?Yes, usingCustom Categoriesfeature training example content.8.What severity levels used image moderation?Safe, Low, High combined threshold settings (Low, Medium, High) determine action.